---
layout: default
title: Rubric Access and Security
---

# Rubric Access and Security

To preserve the integrity of the benchmark, all **final evaluation prompts** and the official **scoring rubric** are maintained confidentially by a neutral steward such as **ML Commons** or a similar governance body. The steward manages distribution and access so that no participating team can tailor their system to the exact test items in advance.

Only **training-style prompts** used for early experimentation are shared publicly. These examples illustrate the format of tasks, but they do not reveal the held-out evaluation items. The actual tests remain private and are only released after each evaluation round concludes.

By separating public examples from private test sets, the project aims to keep assessments fair while still allowing the community to practice and improve on open prompts.
