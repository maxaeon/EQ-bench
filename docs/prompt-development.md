---
layout: default
title: Evaluation Prompt Development
---

# Evaluation Prompt Development

This document outlines how evaluation prompts are designed for the EQ benchmark. Each prompt is explicitly aligned with one or more SERA-X constructs so that responses can be measured against clear objectives. Domain experts review every draft for clarity and cultural sensitivity. After small-scale trials, we refine the wording based on observed results and community feedback, repeating this process until the prompt reliably assesses the intended construct.

**Example**

> *Sensing:* "Describe how you would recognize signs of frustration during a customer support chat."

The example above demonstrates how a single prompt targets the Sensing construct. Expert reviewers validate that it references relevant emotional cues, and we iteratively adjust the phrasing after testing with different models to remove ambiguity or bias.
